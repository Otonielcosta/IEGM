# etapa 1 - Otimizar os hiperparâmetros. 

library(dplyr)
library(rpart)
library(rpart.plot)
library(Metrics)
library(mlr)
library(ggplot2)
library(plotly)

load("iegm17a.RData")
iegm17a <- iegm17a %>% 
  mutate(iEduc=as.factor(iEduc)) %>% 
  mutate(iSaude=as.factor(iSaude)) %>% 
  mutate(iPlanejamento=as.factor(iPlanejamento)) %>% 
  mutate(iFiscal=as.factor(iFiscal)) %>% 
  mutate(iAmb=as.factor(iAmb)) %>% 
  mutate(iCidade=as.factor(iCidade)) %>% 
  mutate(iGovti=as.factor(iGovti)) %>% 
  mutate(IEGeral=as.factor(IEGeral)) %>% 
  mutate(Parecer=as.factor(Parecer))

indiceiegm <- c("A", "B+", "B", "C+", "C")
indiceiegm1 <- factor(indiceiegm,
                      levels = c("A",
                                 "B+",
                                 "B", 
                                 "C+", 
                                 "C"))
julgamento <- c("Favoravel", "Desfavoravel")
julgamento1 <- factor(julgamento,
                      levels = c("Favoravel", 
                                 "Desfavoravel"))

set.seed(123)
bool_treino <- stats::runif(dim(iegm17a)[1])>.25
treino <- iegm17a[bool_treino,]
treino
length(treino)
teste  <- iegm17a[!bool_treino,]
iegm17a %>% str

d.tree = rpart(Parecer ~ iEduc + iSaude + iPlanejamento + iFiscal + iAmb + iCidade + iGovti + IEGeral, 
               data=treino, 
               method = 'class')


predicted_values <- predict(d.tree, teste, type = 'class')
accuracy(teste$Parecer, predicted_values)
d.tree.custom = rpart(Parecer ~ iEduc + iSaude + iPlanejamento + iFiscal + iAmb + iCidade + iGovti + IEGeral, 
                      data=treino, 
                      method = 'class'
                       )

getParamSet("classif.rpart")

d.tree.mlr <- mlr::makeClassifTask(data=as.data.frame(treino[,3:11]),  
                                   target="Parecer"
)

d.tree.params <- mlr::makeClassifTask(data=as.data.frame(treino[,3:11]),  
                                      target="Parecer"
)

param_grid <- makeParamSet(makeDiscreteParam("maxdepth", values=1:30))

control_grid = makeTuneControlGrid()
resample = makeResampleDesc("CV", iters = 3L)
measure = acc

set.seed(123)
dt_tuneparam <- tuneParams(learner="classif.rpart", 
                           task=d.tree.params, 
                           resampling = resample,
                           measures = measure,
                           par.set=param_grid, 
                           control=control_grid, 
                           show.info = TRUE)

result_hyperparam <- generateHyperParsEffectData(dt_tuneparam, partial.dep = TRUE)

ggplot(
  data = result_hyperparam$data,
  aes(x = maxdepth, y=acc.test.mean)
) + geom_line(color = 'darkblue')

dt_tuneparam

best_parameters = setHyperPars(
  makeLearner("classif.rpart", predict.type = "prob"), 
  par.vals = dt_tuneparam$x
)

best_model = train(best_parameters, d.tree.mlr)

d.tree.mlr.test <- makeClassifTask(data=as.data.frame(teste[,3:11]),  
                                   target="Parecer",  
)

results <- predict(best_model, task = d.tree.mlr.test)$data

accuracy(results$truth, results$response)

param_grid_multi <- makeParamSet( 
  makeDiscreteParam("maxdepth", values=4:20),
  makeNumericParam("cp", lower = 0.001, upper = 0.01),
  makeDiscreteParam("minsplit", values=1:10),
  makeDiscreteParam("minbucket", values=1:10)
)

start_time <- Sys.time()

dt_tuneparam_multi <- tuneParams(learner='classif.rpart', 
                                 task=d.tree.mlr, 
                                 resampling = resample,
                                 measures = measure,
                                 par.set=param_grid_multi, 
                                 control=control_grid, 
                                 show.info = TRUE)

end_time <- Sys.time()

end_time - start_time


best_parameters_multi = setHyperPars(
  makeLearner("classif.rpart", predict.type = "prob"), 
  par.vals = dt_tuneparam_multi$x
)

best_model_multi = train(best_parameters_multi, d.tree.mlr)

results <- predict(best_model_multi, task = d.tree.mlr.test)$data

accuracy(results$truth, results$response)

result_hyperparam.multi <- generateHyperParsEffectData(dt_tuneparam_multi, partial.dep = TRUE)

result_sample <- result_hyperparam.multi$data %>%
  sample_n(200)


hyperparam.plot <- plot_ly(result_sample, 
                           x = ~cp, 
                           y = ~maxdepth, 
                           z = ~minsplit,
                           marker = list(color = ~acc.test.mean,  colorscale = list(c(0, 1), c("yellow", "blue")), showscale = TRUE))
hyperparam.plot <- hyperparam.plot %>% add_markers()
hyperparam.plot

############################################################################################################################################ 
#Etapa 2 - gerar a árvore

pacotes <- c('tidyverse',  # Pacote básico de datawrangling
             'rpart',      # Biblioteca de árvores
             'rpart.plot', # Conjunto com Rpart, plota a parvore
             'gtools',     # funções auxiliares como quantcut,
             'Rmisc',      # carrega a função sumarySE para a descritiva
             'scales',     # importa paletas de cores
             'caret'       # Funções úteis para machine learning
)

if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
  instalador <- pacotes[!pacotes %in% installed.packages()]
  for(i in 1:length(instalador)) {
    install.packages(instalador, dependencies = T)
    break()}
  sapply(pacotes, require, character = T) 
} else {
  sapply(pacotes, require, character = T) 
}

library('pROC')

set.seed(123)
arvore <- rpart(Parecer ~ iEduc + iSaude + iPlanejamento + iFiscal + iAmb + iCidade + iGovti + IEGeral,
                data=iegm17a,
                parms = list(split = 'gini'), # podemos trocar para  'information'
                method='class', # Essa opção indica que a resposta é qualitativa,
                xval=5,
                control = rpart.control(cp = 0.01, 
                                        minsplit = 5,
                                        minbucket = 10, 
                                        maxdepth = 6)
)
arvore
paleta = scales::viridis_pal(begin=.75, end=1)(20)
rpart.plot::rpart.plot(arvore,
                       box.palette = paleta)
acctr <- 1:6
sensitr <- 1:6
espectr <-  1:6
acctes <- 1:6
sensites <- 1:6
espectes <- 1:6

p_treino = stats::predict(arvore, treino) 
p_teste = stats::predict(arvore, teste)


k <- c(3.9,4.1,6.2,7.3,8.4,9.9)
for(i in seq_along(k)) {
  k[i] <- k[i]/10
  c_treino = base::factor(ifelse(p_treino[,2]>k[i], "Favoravel", "Desfavoravel"))
  c_teste = base::factor(ifelse(p_teste[,2]>k[i], "Favoravel", "Desfavoravel"))
  
  tab <- table(c_treino, treino$Parecer) #matriz de confusao de treino. 
  acctr[i] <-   if (nrow(tab) < 2) { tab[1,2]/nrow(treino) } else {(tab[1,1]+tab[2,2])/nrow(treino)} #acurária de treino
  sensitr[i] <- if (nrow(tab) < 2) { 0 } else { sensitivity(tab) } #sensibilidade de treino
  espectr[i] <- if (nrow(tab) < 2) { 0 } else { specificity(tab) } #especificidade de treino
  tabtes <- table(c_teste, teste$Parecer) #matriz de confusao de teste 
  acctes[i] <- if (nrow(tabtes) < 2) { tabtes[1,2]/nrow(teste) } else {(tabtes[1,1]+tabtes[2,2])/nrow(teste)} #acurária de test
  sensites[i] <- if (nrow(tabtes) < 2) { 0 } else {sensitivity(tabtes)} #sensibilidade de teste
  espectes[i] <- if (nrow(tabtes) < 2) { 0 } else {specificity(tabtes)} #especificidade de teste
  
}
pontos_corte <- 1:6

for(i in seq_along(k)) {
  pontos_corte[i] <- k[i]*100
  Percentuais_de_Corte <- paste(pontos_corte,"%", sep="")
}

df <- data.frame(Percentuais_de_Corte, acctr, sensitr, espectr, acctes, sensites, espectes)
df
###################################################################################################################################
#Etapa 3- Curva Roc

p_treino = stats::predict(arvore, treino)
c_treino = base::factor(ifelse(p_treino[,2]>.84, "Favoravel", "Desfavoravel"))
p_teste = stats::predict(arvore, teste)
c_teste = base::factor(ifelse(p_teste[,2]>.84, "Favoravel", "Desfavoravel"))

tab <- table(c_treino, treino$Parecer)
tab
tabtes <- table(c_teste, teste$Parecer)
tabtes

aval_treino <- data.frame(obs=treino$Parecer, 
                          pred=c_treino,
                          Favoravel = p_treino[,2],
                          Desfavoravel = 1-p_treino[,2]
)
twoClassSummary(aval_treino, lev=levels(aval_treino$obs))

# Podemos usar o mesmo dataframe para fazer a curva ROC:
CurvaROC <- ggplot2::ggplot(aval_treino, aes(d = as.numeric(factor(obs, levels = c("Favoravel", "Desfavoravel"))) - 1, m = Desfavoravel, colour='1')) + 
  plotROC::geom_roc(n.cuts = 0) +
  scale_color_viridis_d(direction = -1, begin=0, end=.25) +
  theme(legend.position = "none") +
  ggtitle("Curva ROC - base de treino")
CurvaROC
auctr <- pROC::roc(response = treino$Parecer, predictor=p_treino[,2]) 
auctr
############################################
# Avaliar a árvore na base de teste
# Vamos calcular a área da curva ROC com uma função no Caret
# A função é o twoClassSummary, que espera como entrada um dataframe com esse layout:
# obs: uma coluna contendo um fator com as classes observadas
# pred: fator com as classes preditas
# <classe 1> (Y no caso): contém a probabilidade da classe 1
# <classe 2> (Y no caso): contém a probabilidade da classe 2
aval_teste <- data.frame(obs=teste$Parecer, 
                         pred=c_teste,
                         Favoravel = p_teste[,2],
                         Desfavoravel = 1-p_teste[,2]
)

twoClassSummary(aval_teste, lev=levels(aval_teste$obs))

# Podemos usar o mesmo dataframe para fazer a curva ROC:
CurvaROC <- ggplot(aval_teste, aes(d = as.numeric(factor(obs, levels = c("Favoravel", "Desfavoravel"))) - 1, m = Desfavoravel, colour='1')) + 
  plotROC::geom_roc(n.cuts = 0) +
  scale_color_viridis_d(direction = -1, begin=0, end=.25) +
  theme(legend.position = "none") +
  ggtitle("Curva ROC - base de teste")
CurvaROC
auctes <- pROC::roc(response = teste$Parecer, predictor=p_teste[,2]) 
auctes
###################################################################################################################################
